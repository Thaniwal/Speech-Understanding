 OK. All right. So, we discussed about how the speech, how these speech sounds are articulated, how are they actually produced with different wave vibrations and movements of the human body parts. How do we analyze the acoustic properties of this sound? So what happens is by these different movements, there's some sound waves that are generated. What do you have to do? You have to acquire these sound waves. You have to capture these sound waves, digitize them and then perform whatever sort of processing you want to perform. There can be a lot of different kinds of processing that need to be performed. You have to perform these processing and we'll see some of the variations. And throughout the course, it will be how to process these audio sounds. And then what happens? After all of this processing, in case you want to hear back the sound, let's say I wanted to make the sound more melodious, increase the volume, add some kind of audio processing to it and hear it back. What do I need to do then? You had the digitized signal. You converted back to analog. You converted back to sound waves, played. That is how we hear it. So how do you hear it? That depends on your shape, processing of the year and all of that. And how everybody perceives the sound is also relative. For you, something, a DJ music playing in a club might be awesome. Given my age, I might be saying, let's go back home. So it's all very relative. Because our body shapes are different. How we perceive things, our experiences are different. Behaviors are different. So depending on all of that, sound is also perceptive. So how we produce sound, how we hear sound, that is perceptive. So the same was when you speak could be different and the way I speak could be different. Not only accent, but just the audio characteristics itself. So we'll very briefly discuss auditory phonetics and then we'll move on to acoustics. So auditory phonetics is a branch of phonetics that studies how speech sounds are to see the process by the human auditory system. And it focuses on the linguistics and the physical properties of this linguistic signals that we perceive. And then how do we perceive different phones and different kinds of words and all of that? So how the pitch is varying, how the loudness is varying, how the frequency is varying, all of that is something that goes in this auditory phonetics. So design of those earplugs, design of earplugs or design of devices for hearing aid or your headphones, all of that, that comes under auditory phonetics combined with your acoustics and the first part of it. So in order to, when you go from capturing to processing and playing it back, it's the entire field that you have to look at. Computer scientists, signal processing people, machine learning experts, we primarily focus on the center part, which is my acoustics. So how do we process that? Once the data is captured, how do we process that? So in terms of processing, there are some very fundamental properties of it. What are the fundamental properties? Frequency, how do we measure that? What is the amplitude? What is the length of the duration of that? And then I will go into the details of this in a short while. What is frequency? Let me first, let me go further. Okay. Ma'am, I think slides are not changing for us. Slides are not changing? Yeah. You are in the requirement slide? Okay, wait a second. I am also not get the class code. Can you ping in the good chat? So I can add my... Yes ma'am. Yes ma'am. I think I had shared the last lecture. Okay. And it is moving now. Yes ma'am. Thank you. Thank you for raising that. Okay. So when we talk about frequency, we discuss what we talk about is how do you capture audio? Let me... Oh, sorry. Huge slides for this. Okay. How do we capture these sound waves? Digitization, right? You capture that and then you... How do you start processing the sampling and quantization? These are the two processes, right? How do we perform sampling and quantization? What are the different... Very... Getting out of the scale, the digital scale. So for example, there's some sound that you can drop it down, maybe like an activator. No, I'm at a very basic level right now. I'm at a very basic level. I'm not doing that. What is that? So ma'am, actually continuous signal is coming and at a fixed rate, we are actually capturing... So when a signal is coming, at fixed intervals of time, you capture the signal. You capture the signal, right? This is analog signal. You are converting it into a discrete signal right now. So at every interval of time, you are capturing what is the audio? What is the sound level? Right? When I say sound level, what do we mean? What is the amplitude of that sound? Right? So that is called as sampling, right? So it converts a real time... It converts a time varying signal into a discrete time signal, which is consisting of real numbers, right? And what is your quantization? When we do quantization, it replaces that real number that we have with some discrete values, right? Now both of these are subjective. Hi, ma'am. I'm sorry. And quantization. In terms of sampling, how do you decide at what interval would you like to sample a signal? Right? At each interval, that each interval defines what is your... Sampling rate. Ma'am, one thing I want to know, like why we need sampling basically for a continuous signal? Can somebody tell me why do we need sampling at a continuous interval? See, you can work with limited data and what you are doing is you want to quantize it. Sorry, you want to digitize that signal, right? So either you can... It's an analog signal. So you have samples, some audio signal at every time instance. So you have to capture that time instance. Now if you want to capture... We did not want to work on continuous phase. We want to work on some time difference. So time difference you can choose, right? Because for time difference you can choose how much of time difference do you want to keep? If you want to capture an audio... If you want to capture the value of the sound at every microsecond, millisecond, nanosecond, whatever it is, you can do that, right? So sampling rate depends on that. Give me one second, guys. There's some trouble with that. Yeah. So sampling rate is the number of samples per second that you capture to create that waveform, right? And if you create... If you keep too much gap, let's say you are capturing the value, one value every millisecond, right? If you are capturing one value every millisecond, what will happen? In a second? One... Every millisecond. You are capturing one audio value every millisecond, right? Versus your capture... Right. ...you at every nanosecond. So in between, if I take a time frame of nanosecond, so between one nano to two nano, we are missing some information, right? Exactly. Now, between one nano to two nano, you are missing information. But when you come to comparative... One millisecond to the second millisecond, then the amount of information that you are missing is significantly higher, right? So compare it to your visual experiences, right? You have televisions or you have screens, right? When you take a look at TV, what is the resolution of TV? Can somebody tell me? It is a 4K resolution. It comes with high definition, it comes with true high definition, it will be 2K, it will be HD. What happens in all of these? What happens in all of these? Take a look at the televisions that were there earlier or maybe just 10 years back versus the televisions that you have now. So what used to happen is the depth of the pixels. So what we are doing? Take a high resolution image. What we are doing to represent a pixel, we are using a lot more information, right? Versus if you are using very little information to represent a pixel. How much of detailed information can it capture? So if you go in a television showroom, Sonika showroom, Chhehar Makhain Hai, right? If you go to the showroom and just look at the TVs across, right? Ask him which is the highest definition television that you have, the highest resolution television you have versus the lowest resolution television that you have. Even if there is a drop of water falling, you can see, right? You can see the micro movements of water particles and what is happening. If you look at the television that was there earlier, right? It was the RT televisions we had. After that, the televisions that are, that are smaller ones, not very high definition televisions, lower resolution. The clarity of the pixels there is not so high, right? Now take this example to audio. Something playing on your mobile phone, a normal mobile phone, right? What is the same thing that is playing in a theater or you have a very fancy sound system where every drop of water falling down can be heard, right? Every small variation in the audio sound, sound, the kind of music that the air Rahman generates. If you hear that music on a very normal mobile phone versus you hear that music on a Bosca very sophisticated speakers and all, you will see, you will hear the difference in each node, in each variation that is there, right? So, this difference is because of sampling rate or like pixels in the system. See pixels are there in the images. Yeah. So, sampling rate is different, right? Sampling rate is very high, right? So, you are capturing those variations in greater detail, right? But what you have to also understand is your, so this is an example of sampling rates, right? So, this is your analog wave that you see on the left hand side and the corresponding digital result of digitizing that, right? Now, if you have the first figure that the first digital result that you see is you will have breaks in the middle. So, the audio that you will hear will not be as smooth, right? Whereas, if you go to the very bottom figure, you will have very detailed audio compared to the first one. You have a lot more nuances that are there. Now, in order to have a smooth variation, smooth audio signal, what you need is, you need to be able to hear the sound. You need to be able to have a comparative to what your ears can hear, what your ears perceive, right? Take a look at this signal and the more the higher, sorry, if you have higher lower sampling rates, the error in the reconstruction will be higher, right? The smaller the sampling rate, because you are capturing smaller values, right? If you are capturing smaller values, then the error in reconstruction might be higher, because there are lot of samples, there are lot of signals that you are not able to capture. Yes. Why do you capture in? Okay, so the question is why do you capture it in equal intervals? Why the sampling rate is fixed? Why don't you vary it? Any thoughts on that? Anyone? Because like demodulation process will be easy. The demodulation process is also easy when the sampling rate is constant. The process of what is easy? Demodulation means reconstruction of signal again in the process of demodulation. Reconstruction is easy when the sampling rate is fixed. You were saying something? I was saying that if you want to keep the sample, the sample is the same portion of the sample that is being used. See and if you don't keep the sample fixed, the kind of audio that you are getting will be very, very choppy or noisy. You are not getting even information. So, the point is correct. What he is saying is if you are capturing audio stream which is where there is long periods of no audio, in that can you actually sample at a lower rate versus when you have more detailed audio sample at a higher rate. So, you can do that but the way to address it is you filter it out. You capture it, you filter it out. And when you are capturing it at actually, see theoretically speaking you can sample it at different rates also. But then when you are reconstructing it and doing any kind of processing, you need to be aware of what was the sampling rate during these, these, these parts of the signal. It is also possible. See when we, when we utter any sentence or when you are singing a song or any kind of audio signal that is happening, the rate of flow of audio is not the same throughout. Not only the quiet periods but even the, even the, let us say when I am speaking, maybe in the beginning of the class I am going slow, slow, slow. But then towards the middle I have increased my, my speed is increased. And then when I am towards the end of the class I am again slowing down. When you hear a song, again I will go back to A.R. Rahman. It will be like and then it will pick up and then it will slow down. So you can capture all of those variations. It is possible, theoretically not limiting. But then the, the experience that you get, right, the perception that you get by hearing it at a, at a constant rate, at a constantly sampled rate. That versus if you, if you sample it out at, at different phases, then how does it affect? Either you have to very specifically know these are the time zones when there is silent periods. The better is you analyze those frequencies, you analyze the amplitude and cut it out. Filter it, filter out those signals that there is no audio in there, filter out and there is this process, the remaining of that. But when we are capturing, during capturing we do not know what kind of sounds are there, right? So then we capture at a fixed sampling rate. Okay, so depending on what is the requirement, what is the use case, why you are sampling it? So we sample it at different rates. For example, when we were talking about walkie-talkie, wireless intercoms, microphone, phone and all of that, we sample it at 8 kilohertz. Modern equipment, 16 kilohertz, right? Audio CDs or lower quality MPEG audio, 22.05. Whereas audio CD when you have better quality MPEG 1 audio or other MP3, other kinds of audio, which are, which are better quality. You increase that, right? And then you can further increase it. You can in high definition, 48 kilohertz, high definition audio, 96 kilohertz, right? So depends on what is, what is our use case, what are we processing it for? And depending on that, it changes, right? So related to the compression also. Go back and sometimes you have compression also, right? You do, oftentimes you would have done compression, even when you transfer a file on any social media, WhatsApp or email or any of these sort of things, you have a large audio file, right? If you transfer it, right? It gives you an option of, what does it give in order of compressing? You want to send it at the same size, small size, medium size, Google gives you these kinds of options. So it compresses. If you say I want to send it at a small size, it compresses the audio signal. How does it do that compression? This is one of the ways. This is one of the ways, right? You can do the... This sampling helped me compression also, right? Yes. To reduce the information and then we reconstruct. Yes, that is what I'm saying. Sampling will help you in. So if my audio was actually captured at 44 kilohertz and the size is very high because this is probably a song, right? If this is how it was captured, I want to transfer it, I'm bandwidth 9. So this is what your... This is one of the ways. This is your compression algorithms will work. Sampling comfort. Right? Another way is... It depends on what kind of audio it has, right? So all of these different kinds of algorithms can be applied. These can hear frequencies above... About 20 hertz, right? So 20 hertz to I think 2 kilohertz. 28, 20 kilohertz, right? That is what humans hear. So generally when we do sampling something for ease of perception, ease of reception for human ears, we sample it at 40,000, right? We sample at 40 kilohertz. Why do we sample at 40 kilohertz? Because you have that Nyquist theorem sitting there, right? So what does Nyquist theorem say? Nyquist theorem say to accurately represent a signal, the sampling rate must be at least twice the highest frequency present in the signal. 20 kilohertz is what we can hear. So even if the signal contains more than that, we will not be able to hear that, right? So if we are not able to hear that, we consider that as the highest point and we sample it at double the rate, right? So from 20 hertz to 20 kilohertz, the sampling, the minimum sampling that we generally for our purposes it is operated with is 40 kilohertz. For phone and all, we process it at lower because of a lot of issues with respect to bandwidth and processing and all of that. Because you are doing continuous transmission, because of those processes, we do that, right? And then the you can, when you will relate to the quality of audio that you hear on normal points versus when you hear these music on any of these sort of things, right? So that is with respect to sampling, right? What was the second point? The second point was with respect to quantization, right? You have these sampling to data capture career. Now, how do you represent this data? How do you import this data? Import it, right? You can have it in 8 bit. Again, I am relating it to images because probably that is, you would have probably dealt with images more than basic audio signal processing. Images are used on social media, cameras and all of those things keep happening. So it depends on what are you using for encoding? How many pixels? What is the depth level that you are using for encoding? If you use greater depth of these pixel values, the variations that will be there will be very high that will get encoded. Because if you are using just one byte to represent, the variations that are happening at a much micro level will not be able to encode that, right? So quantization will happen accordingly. A lot of pixels, a lot will get the groupings that will happen will happen at a very micro level, right? At a very macro level versus if you have higher depths, then you can encode very, very smaller variation, very unique, very minute variations in the audio and get to get to hear what is happened, right? So these are then two different sort of things, sampling and quantization which you need to keep in mind, right? So when we talk about sampling and quantization, then we then amplitude is another thing that comes, right? What is amplitude? Amplitude is the height of a sound wave, right? So it is measured in decibel and it is a logarithmic scale in which we measure essentially the sound pressure relative to the reference value. Reference value is generally the reference value of quietness that we hear. For quietness that amplitude is considered to be 0, right? So if it is quiet, then that reference value becomes 0 for us, right? The amplitude of that becomes 0 for us. The pressure is, let's say, consider pressure, the baseline pressure is considered as 1. We are dealing with log scale, so base pressure is considered as 1. You have amplitude, we operate in log scale. So amplitude, the value of amplitude increasing by 1. It's not simply just 1. It's log of 1, right? Not log of 1. So the log of a value is increasing by 1. So the significance is lot more, right? Significance of that is lot more. It's not just one value increase, right? And when you see the waveforms, this is how you will see the waveform. So the one at the bottom is representing something with a low amp versus the figure on the top is representing a high amplitude, right? Now this is very fundamental of amplitude. Now there is another thing. There is amplitude. There is loudness, right? Loudness, loudness, I have a lot of voice over here. There is a voice over here, right? What do you mean by face voice? This is something we say, right? The other thing is, TVK has got a volume comfortable. We don't say amplitude come comfortable or we don't say loudness come comfortable. We say volume come comfortable, right? So you have loudness, you have amplitude, you have volume, right? So volume is something that you can control from there. Amplitude versus audio. What is the difference between amplitude and audio? No. Amplitude and loudness. Loudness is? Loudness is directly proportional to amplitude. Loudness is directly proportional to amplitude square. What does it measure? Energy of. Energy of. Energy of. Energy of. Energy of. Energy of. Energy of. Energy of. Energy of. So, right? And what it is also representative of? It is, it in some way measures its frequency also plays a role in loudness, right? Frequency also plays a role in when you are capturing loudness. So, it is a combination of amplitude and the effect of frequency, right? So loudness is, it refers to how we have perceived the intensity or volume of a sound, right? Like I said in the beginning, the volume, the loudness is different to different people. It might be different for you, it might be different for me, right? So not, it depends not only on the physical characteristics, but also how we interpret, right? So you can measure it, but it is difficult to interpret, right? So, yeah, the example I gave earlier, for you it could be not loud, for me it could be very, very loud, right? So similarly, the volume, when you, when you measure the volume, it can have different impact, right? And when we come into a bit more into, into what we mean by decibel, so doubling the intensity of sound means an increase of a little more than three decibels. It is not only directly, directly encoding the volume, it is also because these are sound waves, right? So, waves are being converted into digitized form in what we measure as amplitude, right? So the pressure that it generates, the loudness, because of the loudness, the pressure that is getting generated in the atmosphere measured relative to the reference value that we have, right? So, doubling the intensity of sound means an increase of a little more than three dB, but a hundred times louder sound will have a decibel of twenty. So, twenty decibel doesn't really mean that it has a sound is twenty times louder, whereas it has a lot more significance of its hundred times actually louder. So, relate the decibel value accordingly, right? A sixty dB or so, relate the units in that way. For example, if you have these values here up to first, so you have, I can't see this, sorry. If you have a whisper, this will be like twenty to thirty decibels. If you have a quiet room, around forty. If you have a light rainfall, around fifty. Dishwasher, sixty. Vacuum dry cleaner, seventy. And whereas if you have a car stereo or a jackhammer or a metal concert, the values that these will generate are very, very high. So, it's going up to hundred and fifty decibels. So, hundred times louder sound was will lead to twenty decibel. Imagine what is a hundred and fifty decibels, right? How often do you want to get exposed to it? Is there, is there observation that people who perform these metal concerts? Do they get hearing problems sooner? I don't know, I'm just wondering right now. They do? Okay. They do. So, they do have. They do have. Okay, so you need to have those equipments as well. But then if you have those equipments, I don't know, how do you feel the energy of that? Or, yeah, so I'm sure this is a big area of research that how do you actually protect the people who are performing these? Like, like the sports, sports in the region, sports medicine is a huge area. I'm sure this area of improving the health of musicians and artists who are performing these would also be of significant interest. Very, very selected audience. But yes, they are there, right? So, that is your decibel and the impact of different, different decibel values that you can have. Sorry. Okay, so that was your decibel, right? The second thing is length, right? What do we mean by the length of an audio? Duration. Duration. Duration. Duration. Duration. In some ways, duration, right? So, length of an audio refers to the duration of the audio signal. But it is typically measured in normal units of time. You will have seconds, length, minutes, hours, right? Easy to measure. But then again, depending on the length, what is the unit? What was your sampling rate in which you actually measured? It will depend on that, right? So, length is relative and then it is phonemic in many languages. So, when we say phonemic in many languages, it is possible that, so if it is a longer word, more consonants and vowels, right? It should take more time to pronounce, right? If it is a longer word or if it is a smaller word, it should take smaller time to, reduced time to pronounce ideally, right? In a given language, if you are speaking at a constant rate, it does. Many of the languages, it does. Not every, but many of the languages, it does, right? For example, if you take English, not every appearance of the same consonant or vowel is pronounced in the same way, right? For example, you have examples from the Amitabh Bachchan movie, right? Many of you remember that statement by Amitabh Bachchan or that is too old for you guys. There is a movie, Nama Khalal. Anybody has seen? What is the, what is the statement there? So, there is a dialogue and he says English is a very funny language. Why is it funny? It is a beauty word, but it is divided into beauty. Right? Geo gore, tio, tu, dio doom, right? So, it is o, d to dee, d to dee, but the usage of o, u, these vowels are not consistent. As a Hindi no o ho ga to usko o hi bola jaega. O aan hi van jaega. Yeah, u nahi van jaega, right? But then English is, English is not a phonetic language, right? Because it is not consistent. The, the composition of the words also determine how it is spoken, right? So, so that is one thing. The other thing is the rate, speech of a sound, of a word or a sentence. When we speak it, it is not constant. When I am, when I am angry or when I am super happy, I might be speaking in a very fast pace. Whereas when I am trying to converse with somebody or, or upset, just upset, not, not angry, right? I might be speaking at a low rate. So the speech rate is also not constant. Plus it sometimes slows down at the end of a sentence and depends on a lot of the people's right, your behavior and other things as well. So, length of an audio, you cannot say that length of an audio, if it is, if it is, if it is sampled at this given rate, it will represent this, this, this only, right? So, there are a lot of subjectivities because we are capturing human audio as well. Now let me go back. So this is what, what did we just discuss? Loudness, volume, decibel and the length. Now what is remaining is your frequency, right? What do we mean by frequency? Frequency measures how many times we capture that, right? Relays to your sampling rate, right? So higher frequency means higher pitched sound and the lower frequency means a lower pitched sound, right? Give an example of higher frequency sound and a lower frequency sound. Screeching noise. Screeching noise, right? So, screeching noise like a few weeks back my office door was making. What was happening? Two metal pieces were colliding when I was opening or closing the door. It was very, very troubling noise that I heard, right? Whistle. So these are your higher frequency sounds. What about lower frequency sounds? Right, so flat noise, drums. So the number of variations or sound wave oscillations that are happening, if it is a flat noise, lower, lower variations would be happening, right? So, take an example of this, bass, guitar, low male voice, any of these would be your low frequency and if you take an example of this, this is obviously your low frequency. This is your high frequency, obviously those kinds of sounds. So, a combination of all of these has an impact on how all of these are generated, right? Now, when we were actually, forget all of these, you get amplitude, sorry, you get frequency, you have the sampling rate at which it is captured, right? And combining all of this, your final audio signal is, you get that final audio signal, right? So, when we are capturing that audio signal, we capture that audio signal in the time domain, the signals that we were seeing earlier, right? But when we are actually processing that, now take a look at that audio signal. You have the amplitude, but how much does the amplitude tell you, right? Amplitude is telling me some of the properties related to, from amplitude what all can you interpret? You can talk about duration. You can, any kind of task that you can perform by just looking at amplitude? Mood of a person. Mood of a person. To an extent using amplitude, but to perform anything more than that, you also need the frequency, right? Not only the audio, that won't just help. We talked about tasks like ASR, right? ASR, speaker recognition, any of these sort of tasks that you want to perform and you want to build a model for speaker recognition using just amplitude. It's not going to work, right? Or speech recognition or speaker recognition, it's not going to work. So, what we do is, we convert it into frequency domain and we work with frequency as well, right? Because that provides you a lot of information and you can also, yeah, that provides you a lot of information and you can also relate it back to phonomics in some way, the other details, other properties that you have, then related to, then how we generate, right? How we generate audio. There are a lot of properties that you can relate to that also when we are talking about this, right? When we go back to, when we process it in frequency domain, right? Hello, ma'am. Okay, so you have, when a signal is there in the time domain, x-axis represents your time and your y-axis represents the amplitude, right? What we do is, we convert it into a frequency domain. When we are operating with any kind of, for acoustic processing, further acoustic processing, any kind of operation that we want to perform, we convert it into frequency domain, right? So, now you can have signals with different frequencies. So, this is an example of a signal with different frequencies. When you convert it into frequency domain, you get that information, right? So, when we talk about frequency domain, what happens is your x-axis will be frequency, y-axis will be, we have the amplitude information available with us, right? Now, what we can do is we can use this amplitude. We have this frequency and amplitude information, frequency domain, now, we can also operate on that. Anybody has performed any kinds of operations in the frequency domain? In image we have performed. In image, okay. What kind of operations are you doing? Like, DFT, discrete Fourier transform, and then we can apply low pass filter, high pass filter. You can perform low pass filtering, high pass filtering, sort of things, right? So, if there are the example that you were, we are alluding to, that if you have periods of silence, if you have periods of silence, you know this is low pass, can I perform any kind of low pass filtering or any kind of filtering to remove those frequencies? I am not here, if there is some very shill noise, I mean, which is kind of, if there is some very shill noise, some very high frequency noise, I do not want that, you can perform different kinds of filtering operations on the signal, right? So, that can be performed. Anything else? No, noise removal. Noise removal can be performed. What else? So, these, some of the operations that you can perform on a very basic level with frequency information, right? But what are other things also is, when we are analyzing the audio signals, we have pitch that you can extract from frequency, we have amplitude, and we have, how do we extract other properties out of it? And then go on to processing it for any other tasks, that can be performed, right? So, maybe I will, I will stop here today, it is around 55, I will stop here today. And then in the next class, we will go on to take a look at what is pitch, how we take this? And then from speech, we had this time domain, right? We just had this time domain, which was 1D. How do we take that to create the frequency domain and then go on to create a spectrogram of 2D information, right? Spectrogram of 2D information and how do we process all of that? So, we will go on to look into some of these, but in the meantime, please go back, read about these, read about frequency, amplitude and decibel and some of these properties, how do we process that? And then we will come back and continue with the class on further analysis of speech spectrogram, okay? Any questions? Class code, sorry. Class code is... Can you just one minute? I am no doubt. I am no doubt. I am no doubt. Okay. So, everyone please join this classroom, the Google link, you are already there, but if there is somebody who is not there, the Google classroom... The Google link is there on the classroom itself. Professor, will you share the recordings in the classroom? Share the... Read your recordings in the classroom. Yeah, I will share the recording. Alright, so, we will see you on Monday. Thank you. Thank you. Thank you all. Thank you. Thank you. Thank you.